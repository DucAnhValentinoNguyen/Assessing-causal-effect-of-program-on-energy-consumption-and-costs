---
title: "Causality analysis"
output: html_document
date: "2026-02-09"
---
```{r setup, include = TRUE, echo = TRUE}
# install.packages("glmnet")
library(tidyverse)
library(dplyr)
library(tidyr)
library(glmnet)
```

```{r loaddata, include = TRUE, echo = TRUE}
# 1. the causal effect of each program (Program A and Program B) on energy consumption and energy costs
# 2. the causal effect of each program (Program A and Program B) on health
# 3. the causal effect of retrofits themselves on energy consumption and energy costs. 
# 4. investigate which type of households responded the most to the programs, 
# i.e. for which the causal effect of the programs on energy consumption and energy costs was the smallest and largest, respectively. 

df <- read.csv("./data/energy_retrofit_panel.csv")
dim(df)
colnames(df)
```

### Identify Variable Groups
```{r dataprep, include = FALSE}
id_vars <- c("household_id", "region", "program_type")
time_var <- "year"
outcome_vars <- c("energy_consumption", "energy_cost", "health_issues", "comfort_score")
# retrofit_type is the endogenous treatment for Task 3
retrofit_var <- "retrofit_type" 

# all other variables are baseline controls (X)
# exclude the ID, Time, Outcome, and Retrofit variables to get the list of Controls
all_cols <- colnames(df)
control_vars <- setdiff(all_cols, c(id_vars, time_var, outcome_vars, retrofit_var))
cat("Number of Control Variables found:", length(control_vars), "\n")
```

### Create 'First Difference' Variables (Outcomes)
```{r firstdiff, include = TRUE, echo = TRUE}
# pivot the outcomes to wide format (columns for 2018, 2020, 2023) and calculate the changes relative to the baseline (2018)
outcomes_wide <- df %>%
  select(household_id, year, all_of(outcome_vars), all_of(retrofit_var)) %>%
  pivot_wider(
    names_from = year, 
    values_from = c(all_of(outcome_vars), all_of(retrofit_var)),
    names_sep = "_"
  ) %>%
  mutate(
    # first Difference in Energy Consumption 
    dY_energy_2020 = energy_consumption_2020 - energy_consumption_2018,
    dY_energy_2023 = energy_consumption_2023 - energy_consumption_2018,

    # first Difference in Energy Consumption 
    dY_cost_2020 = energy_cost_2020 - energy_cost_2018,
    dY_cost_2023 = energy_cost_2023 - energy_cost_2018,

    # first Difference in Health 
    dY_health_2020 = health_issues_2020 - health_issues_2018,
    dY_health_2023 = health_issues_2023 - health_issues_2018,
    
    # change in Retrofit Status (Endogenous Treatment for Task 3)
    # define "Retrofitted" as having a type > 0.
    # check if they switched from 0 (No) to something else.
    is_retrofitted_2018 = ifelse(retrofit_type_2018 > 0, 1, 0),
    is_retrofitted_2020 = ifelse(retrofit_type_2020 > 0, 1, 0),
    is_retrofitted_2023 = ifelse(retrofit_type_2023 > 0, 1, 0),
    
    # the 'treatment' for IV is the CHANGE in retrofit status
    d_retrofit_20_18 = is_retrofitted_2020 - is_retrofitted_2018,
    d_retrofit_23_18 = is_retrofitted_2023 - is_retrofitted_2018
  )

# prepare Baseline Controls (X Matrix)
# ---------------------------------------------------------
# We take the 2018 slice of the data for the controls.
# Note: The raw data has NAs for controls in 2020/2023, so we MUST use 2018.

controls_2018 <- df %>%
  filter(year == 2018) %>%
  select(all_of(id_vars), all_of(control_vars))

# check if there are missing values in controls (imputation might be needed)
na_counts <- colSums(is.na(controls_2018))
if(sum(na_counts) > 0) {
  cat("Warning: Missing values detected in controls. Consider imputation if meaningful.\n")
}

# merge to Create Final DML Dataset
# ---------------------------------------------------------
dml_data <- controls_2018 %>%
  left_join(outcomes_wide, by = "household_id") %>%
  mutate(
    # Define Treatment Dummies for Programs (ITT)
    D_A = ifelse(program_type == "A", 1, 0),
    D_B = ifelse(program_type == "B", 1, 0),
    
    # Create a numeric region variable if needed for clustering
    region_id = as.numeric(as.factor(region))
  )

# inspect
cat("Final DML Dataset Dimensions:", dim(dml_data), "\n")
# head(dml_data)
```


Task 1 (Program A Effect): Estimate the causal effect of living in a "Program A" region vs. "Control" on energy consumption and costs.
(Program B Effect): Do the same for "Program B".

Task 2 (Program A Effect): Estimate the causal effect of living in a "Program A" region vs. "Control" on health.
(Program B Effect): Do the same for "Program B".

Task 3 (The Mechanism): Estimate the effect of the retrofit itself (not just the program) using Instrumental Variables (IV).

Task 4 (Heterogeneity): See who benefits the most (e.g., do low-income households save more?).

Approach: Double Machine Learning (DML) with the Partialling Out approach (also known as Robinson's transformation). 
This is the standard way to get a clean causal coefficient ($\beta$) in a linear setting while using Lasso to control for high-dimensional 
confounders ($X$).

The Logic: Predict the Outcome ($\Delta Y$): 
Use Lasso to predict changes in energy use based on baseline characteristics ($X$). 
Calculate the residuals (what the controls can't explain).
Predict the Treatment ($D$): Use Lasso to predict whether a household is in Region A/B based on baseline characteristics ($X$). 
Calculate the residuals (the "surprise" variation in treatment).
Clean Regression: Regress the outcome residuals on the treatment residuals. 
This gives us the causal effect of the program by partialling out all the confounding factors with the Frisch-Waugh-Lovell theorem. 


## Task 1: Effects on Energy Consumption and Energy Cost
The code defines a DML function and applies it to find the effects of Program A and Program B.
```{r task1, include = TRUE, echo = TRUE}
# Load your clean data if not already in environment
# load("./data/clean_dml_data.RData") 

# ---------------------------------------------------------
# 1. Prepare Matrices for glmnet
# ---------------------------------------------------------
# We need a numeric matrix for X (Features). 
# We use the baseline (2018) controls we identified earlier.

# Create formula for model.matrix (all controls, expanding factors to dummies)
# We exclude ID, year, and outcome variables from X
X_formula <- as.formula(paste("~", paste(control_vars, collapse = "+")))

# Create the Design Matrix X (removing intercept as glmnet adds it)
X_matrix <- model.matrix(X_formula, data = dml_data)[, -1]

# Define Outcome (Change in Energy 2023 vs 2018)
Y_energy <- dml_data$dY_energy_2023

# Define Treatments
D_A <- dml_data$D_A
D_B <- dml_data$D_B

# ---------------------------------------------------------
# 2. Define the DML "(Partialling Out) Function
# ---------------------------------------------------------
run_dml_partialling_out <- function(Y, D, X) {
  
  # Step 1: Lasso for Outcome (Y ~ X)
  # ---------------------------------
  cat("  Fitting Lasso for Outcome Y...\n")
  cv_y <- cv.glmnet(X, Y, alpha = 1, family = "gaussian") # Cross-validation to find lambda
  y_hat <- predict(cv_y, newx = X, s = "lambda.min")      # Predict Y
  y_resid <- Y - y_hat                                    # Get Residuals (Y - Y_hat)
  
  # Step 2: Lasso for Treatment (D ~ X)
  # ---------------------------------
  # Note: Even though D is binary, we often use linear Lasso (family="gaussian") 
  # for partialling out in DML papers (Chernozhukov et al.), but "binomial" is also fine.
  # We use gaussian for stability in obtaining residuals.
  cat("  Fitting Lasso for Treatment D...\n")
  cv_d <- cv.glmnet(X, D, alpha = 1, family = "gaussian") 
  d_hat <- predict(cv_d, newx = X, s = "lambda.min")
  d_resid <- D - d_hat
  
  # Step 3: OLS on Residuals (Y_resid ~ D_resid)
  # ---------------------------------
  cat("Running final OLS on residuals...\n")
  final_model <- lm(y_resid ~ d_resid)
  
  # Return summary
  return(summary(final_model))
}

# ---------------------------------------------------------
# 3. Run Analysis for Program A (exclude Program B regions)
# ---------------------------------------------------------
# To estimate the effect of A vs Control, we typically drop Group B 
# to keep the comparison clean (A vs C).

cat("ESTIMATING EFFECT OF PROGRAM A (Energy Consumption)\n")
subset_A <- which(dml_data$program_type %in% c("A", "C"))
res_consum_A <- run_dml_partialling_out(
  Y = Y_energy[subset_A],
  D = D_A[subset_A],
  X = X_matrix[subset_A, ]
)
print(res_consum_A)


# ---------------------------------------------------------
# 4. Run Analysis for Program B (exclude Program A regions)
# ---------------------------------------------------------
cat("ESTIMATING EFFECT OF PROGRAM B (Energy Consumption)\n")
subset_B <- which(dml_data$program_type %in% c("B", "C"))
res_consum_B <- run_dml_partialling_out(
  Y = Y_energy[subset_B],
  D = D_B[subset_B],
  X = X_matrix[subset_B, ]
)
print(res_consum_B)

# ---------------------------------------------------------
# Effect of Programs on Energy COSTS
# ---------------------------------------------------------
# Define Y vector for costs
Y_cost <- dml_data$dY_cost_2023
cat("ESTIMATING EFFECT OF PROGRAM A (Energy Costs)\n")
# Re-using the function we defined earlier
res_cost_A <- run_dml_partialling_out(
  Y = Y_cost[subset_A], 
  D = D_A[subset_A], 
  X = X_matrix[subset_A, ]
)
print(res_cost_A)

cat("ESTIMATING EFFECT OF PROGRAM B (Energy Costs)\n")
res_cost_B <- run_dml_partialling_out(
  Y = Y_cost[subset_B], 
  D = D_B[subset_B], 
  X = X_matrix[subset_B, ]
)
print(res_cost_B)
```

### Task 2: run Analysis for Health 

```{r task2, include = TRUE, echo = TRUE}
# ---------------------------------------------------------
# Task 2: Program A on Health 
# ---------------------------------------------------------
cat("ESTIMATING EFFECT OF PROGRAM A (Health)\n")
Y_health <- dml_data$dY_health_2023
result_health_A <- run_dml_partialling_out(
  Y = Y_health[subset_A],
  D = D_A[subset_A],
  X = X_matrix[subset_A, ]
)
print(result_health_A)

# ---------------------------------------------------------
# Task 2: Program B on Health 
# ---------------------------------------------------------
cat("ESTIMATING EFFECT OF PROGRAM B (Health)\n")
# We only did A last time. Let's check B.
Y_health <- dml_data$dY_health_2023
res_health_B <- run_dml_partialling_out(
  Y = Y_health[subset_B],
  D = D_B[subset_B],
  X = X_matrix[subset_B, ]
)
print(res_health_B)
```


```{r sensitivity, , include = TRUE, echo = TRUE}
# ---------------------------------------------------------
# SENSITIVITY ANALYSIS: DML with Random Forest
# ---------------------------------------------------------
# Goal: Check if the effect (-99.8) holds if we use a non-linear model (Random Forest) to control for confounders instead of Lasso.
# install.packages("randomForest")
library(randomForest)

# We focus on the main result: Program A on Energy Consumption
subset_sens <- which(dml_data$program_type %in% c("A", "C"))
Y_sens <- dml_data$dY_energy_2023[subset_sens]
D_sens <- dml_data$D_A[subset_sens]
X_sens <- X_matrix[subset_sens, ]

cat("SENSITIVITY CHECK: DML with Random Forest\n")
# Step 1: RF for Outcome (Y ~ X)
set.seed(123)
rf_y <- randomForest(x = X_sens, y = Y_sens, ntree = 100)
y_hat_rf <- predict(rf_y, X_sens)
y_resid_rf <- Y_sens - y_hat_rf
# Step 2: RF for Treatment (D ~ X)
set.seed(123)
rf_d <- randomForest(x = X_sens, y = D_sens, ntree = 100)
d_hat_rf <- predict(rf_d, X_sens)
d_resid_rf <- D_sens - d_hat_rf
# Step 3: OLS on Residuals
sens_model <- lm(y_resid_rf ~ d_resid_rf)
print(summary(sens_model))
```

### Task 3 (The Mechanism: IV with DML)
The Double Machine Learning IV approach. This involves three steps of "cleaning" (Partialling Out) using Lasso, followed by a 2SLS regression.
```{r task3, , include = TRUE, echo = TRUE}
# ---------------------------------------------------------
# TASK 3: IV ESTIMATION (Effect of Retrofit Itself)
# ---------------------------------------------------------
# We focus on the "A vs C" subsample because Program B did not work (and thus would be a "weak instrument").
subset_iv <- which(dml_data$program_type %in% c("A", "C"))
# Define Variables for this subset
Y_iv <- dml_data$dY_energy_2023[subset_iv]       # Outcome: Change in Energy
D_iv <- dml_data$d_retrofit_23_18[subset_iv]     # Endogenous Treatment: Did they Retrofit?
Z_iv <- dml_data$D_A[subset_iv]                  # Instrument: Program A Assignment
X_iv <- X_matrix[subset_iv, ]                    # Controls

# Function: DML for Instrumental Variables
run_dml_iv <- function(Y, D, Z, X) {
  # Step 1: Partial out X from Outcome Y
  cat("  1. Lasso Y ~ X (Outcome)...\n")
  cv_y <- cv.glmnet(X, Y, alpha = 1, family = "gaussian")
  y_hat <- predict(cv_y, newx = X, s = "lambda.min")
  y_resid <- Y - y_hat
  # Step 2: Partial out X from Endogenous Treatment D
  cat("  2. Lasso D ~ X (Endogenous Treatment)...\n")
  cv_d <- cv.glmnet(X, D, alpha = 1, family = "gaussian")
  d_hat_x <- predict(cv_d, newx = X, s = "lambda.min")
  d_resid <- D - d_hat_x
  # Step 3: Partial out X from Instrument Z
  cat("  3. Lasso Z ~ X (Instrument)...\n")
  cv_z <- cv.glmnet(X, Z, alpha = 1, family = "gaussian")
  z_hat_x <- predict(cv_z, newx = X, s = "lambda.min")
  z_resid <- Z - z_hat_x
  # Step 4: 2SLS on Residuals
  # (Regress D_resid on Z_resid to get predicted D_resid_hat, then Y_resid on that)
  # First Stage: Effect of Instrument on Treatment (Compliance)
  first_stage <- lm(d_resid ~ z_resid)
  cat("\n  --- First Stage Summary (Instrument Strength) ---\n")
  print(summary(first_stage)$coefficients)
  # Get fitted values from first stage
  d_resid_fitted <- first_stage$fitted.values
  # Second Stage: Effect of Fitted Treatment on Outcome
  iv_model <- lm(y_resid ~ d_resid_fitted)
  return(summary(iv_model))
}
# Run the IV Analysis
# ---------------------------------------------------------
# Effect of Retrofit Itself on Energy CONSUMPTIONS (IV)
# ---------------------------------------------------------
cat("ESTIMATING CAUSAL EFFECT OF RETROFIT (IV) on ENERGY CONSUMPTIONS\n")
result_iv <- run_dml_iv(Y_iv, D_iv, Z_iv, X_iv)
print(result_iv)

# ---------------------------------------------------------
# Effect of Retrofit Itself on Energy COSTS (IV)
# ---------------------------------------------------------
cat("ESTIMATING CAUSAL EFFECT OF RETROFIT (IV) on ENERGY COSTS\n")
# Using Program A as instrument, just like before
# We use the same subset (A vs C) and same D and Z
result_iv_cost <- run_dml_iv(
  Y = Y_cost[subset_A],      # NEW Outcome (Costs)
  D = D_iv,                  # Same Treatment (Retrofit)
  Z = Z_iv,                  # Same Instrument (Program A)
  X = X_iv                   # Same Controls
)
print(result_iv_cost)
```

### Task 4 (Causal Forest)
This code will:
1, Train a Causal Forest on the "Program A vs Control" data.

2, Calculate the CATE (Conditional Average Treatment Effect) for every household.

3, Plot Variable Importance (which features moderate the effect?).

4, Test if the effect varies by Income (a common policy question).
```{r task4, echo = TRUE, fig.keep = 'none', results = 'hold'}
# ---------------------------------------------------------
# TASK 4: HETEROGENEITY (Causal Forest)
# ---------------------------------------------------------
# Install 'grf' if you haven't already. It's the gold standard for this.
if(!require(grf)) install.packages("grf")
library(grf)
cat("TRAINING CAUSAL FOREST\n")

# 1. Prepare Data (A vs C only)
# -----------------------------
# Simple mean imputation for the matrix
X_matrix[is.na(X_matrix)] <- mean(X_matrix, na.rm = TRUE)
subset_hetero <- which(dml_data$program_type %in% c("A", "C"))
# Outcome: Change in Energy
Y_forest <- dml_data$dY_energy_2023[subset_hetero]
# Treatment: Program A Assignment (ITT)
# (We look at heterogeneity of the PROGRAM effect)
D_forest <- dml_data$D_A[subset_hetero]
# Controls: The X matrix
X_forest <- X_matrix[subset_hetero, ]

# 2. Run Causal Forest
# -----------------------------
# tune.parameters = "all" ensures the best model, but takes a minute
cf <- causal_forest(X_forest, Y_forest, D_forest, tune.parameters = "all")
cat("Causal Forest Trained.\n")

# 3. Assess Heterogeneity
# -----------------------------
# Check if there is significant heterogeneity (p-value for differential effect)
test_cal <- test_calibration(cf)
print(test_cal)

# 4. Variable Importance
# -----------------------------
# Variables selection: which variables determine who saves the most energy?
var_imp <- variable_importance(cf)
# Add variable names
var_imp_df <- data.frame(variable = colnames(X_forest), importance = var_imp) %>%
  arrange(desc(importance))
cat("\nTop 5 Variables driving Heterogeneity:\n")
print(head(var_imp_df, 5))

# 5. Visualize: Effect by Income
# -----------------------------
# Let's see if the effect is different for High vs Low Income
# We predict the treatment effect (tau) for everyone
tau_hat <- predict(cf)$predictions

# Add predictions back to the data frame for plotting
plot_data <- dml_data[subset_hetero, ]
plot_data$tau_hat <- tau_hat

# Plot 1: Histogram of Effects
# (Shows the distribution of savings)
hist(plot_data$tau_hat, 
     main = "Distribution of Treatment Effects (Energy Savings)",
     xlab = "Estimated Effect (kWh)", col = "lightblue")

# Plot 2: Effect vs. Income
# (Smoothed line showing relationship)
# We assume 'income' is in the dataset. If it's scaled/transformed, adjust accordingly.
# Using base R plot for simplicity
plot(plot_data$income, plot_data$tau_hat,
     main = "Treatment Effect by Income Level",
     xlab = "Household Income",
     ylab = "Treatment Effect (Negative = More Savings)",
     col = rgb(0,0,0,0.2), pch=16)
lines(lowess(plot_data$income, plot_data$tau_hat), col="red", lwd=3)

# 6. Shows results for the report
print(summary(plot_data$tau_hat))
```

The "Table Substitution" (Since Plots Failed)
Since the plots didn't show up, we generate a Summary Table that proves the direction of these effects for the report.
```{r tablesub, include = TRUE, echo = TRUE}
# Compare effects for Old vs. New Buildings
# (Using Median split)
med_age <- median(plot_data$building_age, na.rm = TRUE)
cat("Effect by Building Age\n")
print(tapply(plot_data$tau_hat, plot_data$building_age > med_age, mean))
# FALSE = Newer Buildings, TRUE = Older Buildings

# Compare effects for High vs. Low Income
med_inc <- median(plot_data$income, na.rm = TRUE)
cat("Effect by Income\n")
print(tapply(plot_data$tau_hat, plot_data$income > med_inc, mean))
# FALSE = Poorer, TRUE = Richer
```
